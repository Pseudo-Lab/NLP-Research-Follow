## 논문
- 제목 : Relation Classification as Two-way Span-Prediction
- 원본 : [https://arxiv.org/abs/2010.04829](https://arxiv.org/abs/2010.04829)
- 요약 : [PDF](summary/3.%20Relation%20Classification%20as%20Two-way%20Span-Prediction.pdf)
- 논문 Background : relation extraction 성능 평가 데이터셋인 TACRED에서 상위 성능을 낸 접근법 중 유일하게 Span prediction 접근법을 사용했다고 하여 관심이 가 선정하게 되었습니다. 
<br>


## 참여자
이정훈, 구선민, 김소연, 김지성, 남궁민상, 류지은
<br><br>


## 토론
- Q1. 섹션 3에서 $r\in R \cup \left \{ \phi \right \}$ 이라는 표기가 등장하는데, 왜 굳이 이렇게 쓰는지 이해가 가지 않네요. $\phi$ 기호가 'no relation from R holds'라고 쓰여있는데 이게 어떤 뜻일까요? 왜 $\phi$을 더하는 걸까요?

  >- A1-1. 엔티티 간 어떤 관계가 있는지 표시하는 R에서 no relation을 강조하기 위함이 아닐까 싶습니다. Sentence level relation extraction을 할 때 한 번에 하나의 트리플을 추출하는데 대부분이 no relation이라고 합니다! 또한 이렇게 표기해 놓으면 나중에 계산량을 표현할 때 relation + no-relation(1) → R+1로 간단하게 표기하기에도 좋은 것 같습니다.

  >- A1-2. R이 존재 + 합집합 + R이 존재하지 않음(R이 들고있는 것 중에 relation있는게 없음, no relation from R holds)에 r이 subset이다-라는 것 아닐까요.

<br>

- Q2. 섹션 5.2에서, relation과 token variation이 등장하는데요. relation dataset에서 관계명을 미리 지정된 토큰으로 바꾼 게 token dataset인 건가요? 언뜻 봤을 때는 관계의 명칭만 바꾼 것 같은데 (이를 테면 per:title에서 r2라는 식으로), 어디서 모델의 성능 차이가 생기는 걸까요?

  >- A2. 자연어처리에서 stemming과 lemmatazation과 비슷한 느낌인 것 같습니다. 원형 단어로 만들어줘서 최대한 같은 클래스로 묶이게 하는 의도인 것 같습니다.

<br>

- Q3. 성능이 BERT보다 ALBERT가 높은데 그 이유에 대해서는 잘 기술이 안되어 있는 것 같습니다. ALBERT의 어떤 요소가 성능 차이에 영향을 미친 것일까요?

  >- A3. 제 생각에는 BERT와 ALBERT의 사전 훈련 데이터와 환경등이 영향을 미치지 않았을까 싶습니다. 만약, 이 부분을 동일한 데이터로 유사한 환경에서 pre trained embedding을 하지 않았다면 성능에 유의미한 영향을 줄 수 있지 않을까요 싶네요. 

<br>

- Q4. 제가 모델 부분이 잘 이해가 안 가는데 결국은 BERT, RoBERTa 같은 임베딩을 사용해 텍스트를 벡터로 변환시키고 그 뒤에 소프트맥스를 적용한 multi classification 구조인가요?  

<br>

- Q5. 논문의 이미지 구조는 아래 그림과 같이 되어 있습니다. 아래 Span prediction RC 흐름은 꽤나 복잡한 흐름으로 느껴집니다. relation과 관련된 질문 2개를 생성하고 그 두개 답변을 combine하는 과정까지 절차가 많은데 성능이 높은 이유가 무엇일까요?
<p align="center"><img src="https://github.com/vhrehfdl/NLP-Research-Follow/blob/main/season2/img/3-1.png"></p>

  >- A5-1. MTB는 context와 entity를 고려하여 relation을 추출하는데 SP(Span Prediction)는 question을 주고 정답 answer를 추출하는데 question이 중요한 정보? 관심 있는 정보?를 나타내서 answer의 근거를 찾는데 도움이 되는 역할을 해줘서 성능 향상이 된 것이 아닐까 싶었습니다.  

  >- A5-2. 선민님 말마따나, 인풋으로 들어가는 question이 추가적인 정보를 주는 것이 영향을 미쳤을 것 같아요. 또, bidirectional question을 사용하거나 AND / OR 방식으로 대답을 조합하는 등 추가적으로 성능을 올릴 수 있는 장치들이 있어서 퍼포먼스가 향상되지 않았나 싶습니다!

<br>

- Q6. 제가 이쪽 분야는 낯설어서 문제 정의 부분부터 궁금한 부분이 있습니다.  introduction에서 두가지 work을 소개하며  relation classification/extraction의 현재 문제점을 짚고 있는데, 그러한 문제가 발생하게 되는 근본적인 원인은 무엇이라고 생각하시나요?  
1) that 10% of the errors are the result of predicting a relation that is based on other arguments in the sentence  
2) where the authors showed that the current embedding based methods often classify a sentence without considering the marked entities (논문에서 marked entity에 대해 설명을 해주긴 하는데.. 이것도 제가 잘은 이해가 안되네요 ㅠㅠㅠ)

  >- A6. 1) other argument라는 것은 애초에 다른 relation을 predict했다라는 것을 의미하는 것 같습니다. 해당 논문 저자의 주장에 의하면 marked entities를 사용하지 않은 것을 문제 삼는 것 같습니다. 제 생각에는 Task의 성능적 차이가 영향을 미치지 않았을까 싶습니다. 일단 기존 방법인 MTB는 생성방식으로 relation을 만들어내는 것 같습니다. (이게 제가 정확히 이해했는지 모르겠습니다.) Span prediction 방식이 중간에 추가되어 안정적으로 entitiy를 추출하고 그 부분을 일치성을 사용해 relation의 True와 False를 반환합니다.   
2)아래 그림의 NA, October 1976, NA, NA 같은 것이 marked entity라고 생각했습니다.  
Our span-prediction method forces the model to identify the exact entities which compose each relation, which helps the model to overcome the challenges presented in these two works.  
위 문장을 보시면 도움이 될 것 같습니다.

<br>

- Q7. 3.2 implication에서 more demanding loss function이 필요하다, shallow heuristics 문제를 해결하기 위한 loss function으로 설정되어 있다는데,  본 논문에서는 span prediction을 score function으로학습했다..? + 레퍼런스 몇 개로만 언급되어 있어서, 혹시 본 논문이 training 시 기존의 loss function과 다르게 강점으로 갖고가는 점이 무엇이라고 생각하시나요?
(* shallow heuristics : effective for solving many dataset instances, but which may fail on more challenging examples. [[레퍼런스 페이퍼]](https://arxiv.org/pdf/2010.03656.pdf)) 

<br>

- Q8. entity에서 해당하는 모든 relation을 다 해봐야 하기 때문에 시간이 오래 걸리는데 사실 실제 문장에서 엔티티가 3개를 초과하는 경우도 꽤 많이 있을텐데 이럴 경우 연산량이 너무 커져서 실효성 있는 방법일까?라는 생각도 듭니다. 혹시 QA에서는 연산량이 커질 경우 어떤 식으로 조절할까요?  

  >- A8. 전통적인 QA 방법에서 연산량에 가장 크게 영향을 주는 요소는 context의 길이라고 생각합니다. (물론 모델의 파라미터가 비슷하다고 가정할 때 입니다.) 그래서 context의 길이를 조절해주는 부분이 중요하다고 생각합니다. 
<p align="center"><img src="https://github.com/vhrehfdl/NLP-Research-Follow/blob/main/season2/img/3-2.png"></p>

<br>

- Q9. Relation classification이라는 것 자체가 많이 낯설어서 그러는데요. 결국 RC를 사용하는 목적이 QA같은 기능들의 향상을 도모함에 있는건가요??

  >- A9. 엔티티를 인식하고 엔티티 간의 관계를 classification하는 것을 Relation Extraction이라고 합니다. 엔티티들을 기존에 존재하는 knowledge와 연결 시키면 지식 추출이 됩니다. 즉 관계 추출은 지식 추출을 위한 첫단계(?)라고 생각하시면 될 것 같습니다. knowledge graphs가 정교할 수록 QA, MRC 등 다양한 태스크에서 잘 활용할 수 있다고 합니다.

<br>

- Q10. 논문에서 제안한 model architecture를 보면서, GNN 구조가 먼저 떠오르긴하던데 추출된 관계를 graph 형태로 embedding 하여 RC 문제에 접근한 paper가 있을까요 ?! 

  >- A10. [https://aclanthology.org/P19-1128/](https://aclanthology.org/P19-1128/)
