## 논문
- 제목 : Text Summarization with Pretrained Encoders
- 원본 : [https://arxiv.org/abs/1908.08345](https://arxiv.org/abs/1908.08345)
- 요약 : [PDF](https://melon-buffer-f27.notion.site/Text-Summarization-with-Pretrained-Encoders-098920484ee14f1b9a13b0b32f047068)
- 논문 Background : 앞서 재미난 주제와 내용들의 논문들을 뽑아주셔서 어떤 주제를 해야 흥미로울지 고민이 많았습니다. 
PLM, Chat-bot, bias & fairness, relation extraction 등이 주제가 다채로워 다루지 않았던 내용을 다뤄보는게 재미있을거 같아 선택해보았습니다. 
개인적으로 관심있었던 분야이기도 하구요. (참고 링크 : [Text Summarization Repo](https://github.com/uoneway/Text-Summarization-Repo))

<br>


## 참여자
이정훈, 구선민, 김소연, 김지성, 류지은
<br><br>


## 토론
- Q1. 식 (1)~(6)을 보면 LN, MHAtt, FFN, PosEmb 등의 개념이 등장하는데, 제게는 생소한 내용이네요ㅠ 문맥상으로는 벡터를 인풋으로 받는 함수들인 것 같은데 이게 정확히 어떤 변환인지, 그 내용들이 일목요연하게 정리된 아티클이 있을까요? (언급된 것 외에 자주 사용되는 함수가 뭐가 있는지도 궁금합니다!)

  >- A1. 해당 개념들은 'Transformer (Attention is all you need)' 에 사용되면서 model architecture에서 설명합니다. 해당 논문을 읽는게 제일 정확하지 싶어요 그래도 혹시나 참고하실만한 자료 링크 남길게요 ! - LN(Layer Normalization) : Residual Connection 결과를(Multi-head Attention / Position-wise FFNN)의 입력)과 출력을 더해주는 역할) 정규화 하여 보다 수렴이 잘되도록 하는 역할
  - MHAtt(Multi-head Attention) : Multi-head Attention은 Self-Attention을 병렬적으로 사용한 층
  - FFN(Feed-Forward Neural Network)
  - PosEmb(Positional Embedding)

    ![https://user-images.githubusercontent.com/29038531/117391026-23db6480-af2a-11eb-927b-cc1c86491282.png](https://user-images.githubusercontent.com/29038531/117391026-23db6480-af2a-11eb-927b-cc1c86491282.png)
    
    [[1] Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)  
    [[2] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) 
    [[3] DSBA - Transformer 강의](https://www.youtube.com/watch?v=Yk1tV_cXMMU&t=1610s)  
    [[4] 자연어처리입문 - 트랜스포머](https://wikidocs.net/31379)  
    [[5] paper review (1)](https://machinereads.wordpress.com/2018/09/26/attention-is-all-you-need/)  
    [[6] paper review (2)](https://www.notion.so/Attention-is-All-You-Need-d484b19f68a54d589cfdb2d76495c73c) - transformer    
    [[7] paper review (3) - BERT](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-f46d9614a2a64f18b477d4d3c6849730)


<br>


- Q2. LEAD-3의 설명을 들었을 때는 '정말 baseline으로만 쓰이겠구나' 싶었는데 CNN/DailyMail 데이터셋에서는 다른 모델과 비교했을 때 LEAD-3의 성능이 은근히 좋네요...? 
비교한 모델 중 1/3 정도는 LEAD-3보다 F1-score가 낮고요. 이게 기존 모델들의 성능이 나빴기 때문일까요, 아님 이 데이터셋 고유의 특징일까요? 

  >- A2.  저는 데이터셋 고유의 특징이라고 생각합니다. 뉴스 특성 상 앞쪽에 중심 내용이 있는 경우가 많아서 문서 맨 앞의 3문장을 선택하는 LEAD-3의 성능이 높게 나왔다고 생각합니다.!

  >- A3. 저도 데이터셋의 특징이라 생각합니다. 정보 전달과 가독성을 고려하는 '뉴스' 의 특징상 '두괄식 전개구조' 를 많이 쓰기에 가장 앞부분의 문장에 정보를 요약하는 경우들이 많기때문이지 싶어요 

<br>

- Q3. Extractive Summarization 학습에 필요한 데이터 셋은 각 Source 문장에 대해 0과 1로 라벨링이 되어 있는 데이터가 필요한 것 같습니다. 학습 데이터 셋은 직접 구성한 것일까요? 논문에 해당 부분을 발견하지 못해 질문 올립니다~!

  >- A3. 네 맞습니다 ! Extractive Summarization 학습을 위해 어떤 문장이 요약에 포함될지 나타내는 정보가 필요합니다. 해당 기사의 요약문과, 요약문의 index는 사람이 직접 작성한걸로 알고 있습니다. 
  생성요약문과 가장 유사한 원문 본문 내의 문장들을 추출해 선택함으로써 자체적으로 생산하고, 생성 요약문과의 유사성은 ROUGE score를 기준으로 판단하여 사용했다고 하네요 ! 
  (논문에서 사용한 데이터셋과 유사하게 한글 뉴스 Extractive Summarization task의 경진대회가 있었는데 아래 링크의 데이터셋 구성을 보시면 말씀하셨던 칼럼이 따로 있는걸 확인하실수 있습니다 ㅎㅎ) [(DACON - 한국어 문서 추출요약 AI 경진대회)](https://dacon.io/competitions/official/235671/data)

<br>

- Q4. 제가 요약 Task를 잘 몰랐었는데 이번 논문 읽으면서 많이 공부했습니다. 해당 논문에서는 Extractive와 Abstractive 방식을 비교했는데 만약 뉴스 기사 요약 모델을 만들어야 한다면 둘 중 어떤방법은 선택하실껀가요? 

  >- A4. 어떤 파트 (사회 지면 뉴스, 스포츠 지면 뉴스, 기상 뉴스, 경제 뉴스 등등)에 따라 선택지를 다르게 선택할거 같습니다. 신뢰성과 늬양스에 민감한 뉴스라고하면 Extractive 방식을 사용하는것이 Abstractive model 보다 정보 왜곡을 줄일거라 판단 듭니다. 
  (스터디 목표중 하나에 '논문구현' 이 있던데.. 시간이 될지 모르겠지만 한글 데이터셋으로 구현 해보고 싶다는 생각은 있습니다 ㅎㅎㅎ)

<br>

- Q5. extractive summarization 후 abstractive summarization 한 모델의 성능이 좋은 이유는 일반적으로 여러 개의 downstream task를 학습했을 때 성능에 긍정적 영향을 끼치는 것으로 이해하면 될까요? 아니면 같은 요약이라는 같은 task니 다른 관점으로 봐야할까요?

  >- A5. 제 생각에는 extractive summarization을 통해 이미 핵심적인 문장(양질의 데이터)을 추출했고 그 데이터를 기반으로 abstractive summarization을 진행해서 성능이 좋은 것이라고 생각했습니다. 원본 소스에는 도메인과 좀 동떨어져있는 불필요한 문장들이 있을 것입니다.(이미 extractive summarization은 도메인과 관련있는 문장을 고르기 때문) 그런 문장들을 extractive summarization에서 이미 1차 필터링을 해주고 도메인과 관련되 데이터만 있는 상태에서 abstractvie summarization을 적용하면 더 좋을 것이라 생각했습니다.

  >- A5. 유의미한 feature를 select 하고 예측을 진행한것과 비슷한 효과라고 생각합니다! 주제어와 가장 유관한 문장을 선택한것을 바탕으로 문장을 생성하기때문에  보다 유관한 문장들을 구사한점이 좋은 보다 나은 성을 내게한거죠 ㅎㅎ 다만 어떤 평가 지표를 쓰느냐의 차이는 있을거 같습니다. 위의 방법처럼 문장을 생성하게되면 어휘선택의 다양성이 높다곤 볼수 없을거 같다는 생각이 드네요 ㅎㅎ

<br>

- Q6. Extractive summary와 Abstactive summary 중 좀 더 범용성 있는 방법은 어떤 것이라고 생각하시나요?

  >- A6. 아무래도 범용적으로는 extractive 방식이 더 낫지 않을까 싶습니다. 아직까지는 abstractive는 편향되게 나올 수도 있고, 엉뚱한 말을 늘어놓을 수도 있는 등 안정성이나 (물의를 일으키지 않는) 안전 측면에서도 불안한 부분들이 있는 것 같습니다. 

  >- A6. 저도 범용적으로 사용하기엔 아직은 Extractive summary 방식이 낫지 않을까 생각합니다. 아직 생성된 문장에 대해서 문장 구사력과 생성된 문장에 대한 거부감(가독성, 논리적 구조, 글의 늬양스에 맞지 않는 문장)의 한계가 있다고 생각합니다. 이렇게 생성된 문장을 정보로 받아들일지에 대한 사회적합의도 필요하다 생각이 듭니다 ..
