## 논문
- 제목 : RoBERTa: A Robustly Optimized BERT Pretraining Approach
- 원본 : [https://arxiv.org/pdf/1907.11692.pdf](https://arxiv.org/pdf/1907.11692.pdf)
- 요약 : [PDF](https://lee-soohyun.tistory.com/277)
<br>


## 참여자
이정훈, 구선민, 남궁민상, 김지성, 류지은
<br><br>


## 토론
- Q1. 최근 여러 NLP Embedding 기법이 나와있는데요 여러분은 텍스트 분류 모델을 만들어야하는 과제를 받았을 때 어떤 embedding 모델을 사용하실 것 같나요? 현재 나와있는 embedding 기법을 모두 조합해서 실험하기에는 리소스가 크니 후보군을 설정하고 그 안에서 비교할 것 같은데... 그 후보군을 어떻게 조절하실까요?

  >- A1. 규모가 훨씬 작은 토이 데이터셋을 만들고, 그 토이 데이터셋에 대해서 기존 embedding 기법들 중 몇 가지를 추려서 실험을 해보고, 성능이 좋은 임베딩을 쓸 것 같아요. 영어처럼 관련 라이브러리가 많고 비교적 문장 자체를 전처리하는게 쉬울 경우 그 중에서 유명한 것 순으로 추려서(...) 쓸 수도 있겠지만, 한국어처럼 그렇지 못한 케이스에는 그냥 다 테스트 해볼 것 같아요..  

<br>

- Q2. 개인적으로 RoBERTa의 높은 성능은 학습 데이터 양의 증가가 크다고 생각하는데 다른 분들은 어떻게 생각하세요. 기존 BERT와 비교해 다른 기법들이 추가되기도 했지만 그 부분이 실제 큰 건지 모르겠습니다. 

  >- A2. 저도 그렇게 생각합니다. 저자가 BERT-style pretraining 방식이 text의 양이 성능에 큰 영향을 미친다는 논문을 참조(Clozedriven pretraining of self-attention networks)하여 연구하였습니다. 이에 따라 많은 양의 데이터를 모아 훈련시켰기 때문에 높은 성능이 나올 수 있었다고 생각합니다. 

  >- A2. 저도 동의합니다 ! 논문에서도 pre-training 하는 데이터의 양과 다양성이 퍼포먼스에 중요하게 작용했다고 언급 하기도 했고, 기존 모델보다 훨씬(?) 많은 양을 학습에 사용했던점 때문에 hyper-parameter tuning으로 성능이 향상된거라고 말하기에는 모호할거 같습니다. 만약 같은 규모의 데이터셋으로 BERT 모델과 비교를 했다면 어떨지 궁굼 하네요

<br>

- Q3. RoBERTa는 BERT가 undertrained 되었다는 것을 발견하여 연구하기 시작했는데요. 다른 모델들도 이런 관점에서 시작된 연구가 있을까요? 혹은 어떤 모델이 아직 undertrained 되어 있을 것 같으신가요?

  >- A3. GPT2도 GPT1의 성능이 그렇게 좋지 않아 학습 데이터를 늘리고 방법을 변형시킨 것이 비슷한 것 같습니다. 사실 undertrained라는게 단순히 성능이 좋지 않은건지 학습이 덜 되서 loss가 잘 안떨어지는 것인지 명확하게 이해가 가지는 않네요.

<br>

- Q4. 유니코드로 만든 BPE 대신에, bytes-level BPE를 썼다고 했는데요. 여기에 대해서 좀 더 부연 설명해주실 분 계실까요?

  >- A1. Multilingual에서 효과가 극대화 되는 것 같습니다. 유니코드 기반은 character 단위를 의미하는 것이였으면 BBPE는 character를 이루는 byte 단위까지 내려가는 것 같습니다. 그래서 다른 나라의 character들간에도 호환이 되어 multilingual에 효과가 있다고 말하는 것 같습니다.
예를 들어 영어 a와 한국어 ㄱ은 character 기반으로는 전혀 잡아낼 수 없지만 byte 기반으로는 일치하는게 있을 수 있다?인 것 같습니다.
